#[cfg(test)]
mod tests {
    use arrow::record_batch::RecordBatch;
    use arrow::util::pretty::pretty_format_batches;
    use datafusion::prelude::*;
    use crate::query::forked::parquet_table::ParquetTable;
    use std::sync::Arc;

    fn init() {
        let _ = env_logger::builder().is_test(true).try_init();
    }

    #[tokio::test]
    async fn roundtrip() -> datafusion::error::Result<()> {
        init();

        let mut ctx = ExecutionContext::new();
        // The parquet file is generated by the command
        //   csv2parquet testinput/example.csv testinput/example.parquet
        // You can verify its content by the command
        //   parquet-tools show testinput/example.parquet
        // If you specify a folder, it will group all .parquet files in the folder under one table.
        let table = ParquetTable::try_new("testinput", 1)?;
        ctx.register_table("example", Arc::new(table));

        // create a plan to run a SQL query
        let df = ctx.sql("SELECT a, MIN(b), COUNT(*) FROM example GROUP BY a ORDER BY a LIMIT 100")?;

        // execute and print results
        let results: Vec<RecordBatch> = df.collect().await?;
        assert_eq!(
            pretty_format_batches(&results)?,
            "+---+--------+-----------------+\n\
            | a | MIN(b) | COUNT(UInt8(1)) |\n\
            +---+--------+-----------------+\n\
            | 1 | a      | 3               |\n\
            | a | 1      | 3               |\n\
            +---+--------+-----------------+\n"
        );
        Ok(())
    }
}
